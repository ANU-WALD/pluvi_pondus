{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_band_ds(n1,n2):\n",
    "    dsh = xr.open_mfdataset([\"/data/GPM_HIM8/HIM8_201811.nc\",\"/data/GPM_HIM8/HIM8_201901.nc\",\"/data/GPM_HIM8/HIM8_201902.nc\"], combine='by_coords')\n",
    "    dsg = xr.open_mfdataset([\"/data/GPM_HIM8/GPM_201811.nc\",\"/data/GPM_HIM8/GPM_201901.nc\",\"/data/GPM_HIM8/GPM_201902.nc\"], combine='by_coords')\n",
    "    b1 = dsh[f'B{n1}'].values\n",
    "    b2 = dsh[f'B{n2}'].values\n",
    "    y_train = dsg['PrecCal'].sel(time=dsh.time).values\n",
    "    dsg.close()\n",
    "    dsh.close()\n",
    "    \n",
    "    b1_mean = b1.mean()\n",
    "    b1_std = b1.std()\n",
    "    b2_mean = b2.mean()\n",
    "    b2_std = b2.std()\n",
    "    \n",
    "    b1 = (b1-b1_mean)/b1_std\n",
    "    b2 = (b2-b2_mean)/b2_std\n",
    "    \n",
    "    x_train = np.stack((b1,b2), axis=-1)\n",
    "    b1 = None\n",
    "    b2 = None\n",
    "\n",
    "    not_nan = ~np.isnan(x_train).any(axis=(1,2,3))\n",
    "    x_train = x_train[not_nan,:]\n",
    "    y_train = y_train[not_nan,:,:,None]\n",
    "    \n",
    "    dsh = xr.open_dataset(\"/data/GPM_HIM8/HIM8_201812.nc\")\n",
    "    dsg = xr.open_dataset(\"/data/GPM_HIM8/GPM_201812.nc\")\n",
    "    b1 = dsh[f'B{n1}'].values\n",
    "    b2 = dsh[f'B{n2}'].values\n",
    "    y_test = dsg['PrecCal'].sel(time=dsh.time).values\n",
    "    dsg.close()\n",
    "    dsh.close()\n",
    "    \n",
    "    b1 = (b1-b1_mean)/b1_std\n",
    "    b2 = (b2-b2_mean)/b2_std\n",
    "    \n",
    "    x_test = np.stack((b1,b2), axis=-1)\n",
    "    b1 = None\n",
    "    b2 = None\n",
    "\n",
    "    not_nan = ~np.isnan(x_test).any(axis=(1,2,3))\n",
    "    x_test = x_test[not_nan,:]\n",
    "    y_test = y_test[not_nan,:,:,None]\n",
    "    \n",
    "    return x_train, np.clip(y_train,0,30), x_test, np.clip(y_test,0,30)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "def get_band_ds(n):\n",
    "    dsh = xr.open_mfdataset([\"/data/GPM_HIM8/HIM8_201811.nc\",\"/data/GPM_HIM8/HIM8_201901.nc\",\"/data/GPM_HIM8/HIM8_201902.nc\"], combine='by_coords')\n",
    "    dsg = xr.open_mfdataset([\"/data/GPM_HIM8/GPM_201811.nc\",\"/data/GPM_HIM8/GPM_201901.nc\",\"/data/GPM_HIM8/GPM_201902.nc\"], combine='by_coords')\n",
    "    \n",
    "    x_train = dsh[f'B{n}'].values\n",
    "    y_train = dsg['PrecCal'].sel(time=dsh.time).values\n",
    "    dsg.close()\n",
    "    dsh.close()\n",
    "    \n",
    "    not_nan = ~np.isnan(x_train).any(axis=(1,2))\n",
    "    x_train = x_train[not_nan,:,:,None]\n",
    "    y_train = y_train[not_nan,:,:,None]\n",
    "    \n",
    "    dsh = xr.open_dataset(\"/data/GPM_HIM8/HIM8_201812.nc\")\n",
    "    dsg = xr.open_dataset(\"/data/GPM_HIM8/GPM_201812.nc\")\n",
    "    \n",
    "    x_test = dsh[f'B{n}'].values\n",
    "    y_test = dsg['PrecCal'].sel(time=dsh.time).values\n",
    "    dsg.close()\n",
    "    dsh.close()\n",
    "    \n",
    "    not_nan = ~np.isnan(x_test).any(axis=(1,2))\n",
    "    x_test = x_test[not_nan,:,:,None]\n",
    "    y_test = y_test[not_nan,:,:,None]\n",
    "    \n",
    "    return (x_train-x_train.mean())/x_train.std(), np.clip(y_train,0,30), (x_test-x_train.mean())/x_train.std(), np.clip(y_test,0,30)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 512, 512, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 512, 512, 1)  4           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_1 (Conv2D)                (None, 512, 512, 4)  40          batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 512, 512, 4)  16          conv1_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling2D) (None, 256, 256, 4)  0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 256, 256, 8)  296         max_pooling2d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 256, 256, 8)  32          conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling2D) (None, 128, 128, 8)  0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 128, 128, 16) 1168        max_pooling2d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 128, 128, 16) 64          conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling2D) (None, 64, 64, 16)   0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 64, 64, 32)   4640        max_pooling2d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 64, 64, 32)   128         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling2D) (None, 32, 32, 32)   0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 32, 32, 64)   18496       max_pooling2d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 32, 32, 64)   256         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling2D) (None, 16, 16, 64)   0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 16, 16, 128)  73856       max_pooling2d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 16, 16, 128)  512         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_6 (UpSampling2D)  (None, 32, 32, 128)  0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 32, 32, 192)  0           up_sampling2d_6[0][0]            \n",
      "                                                                 conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 32, 32, 64)   110656      concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 32, 32, 64)   256         conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_7 (UpSampling2D)  (None, 64, 64, 64)   0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 64, 64, 96)   0           up_sampling2d_7[0][0]            \n",
      "                                                                 conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 64, 64, 32)   27680       concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 64, 64, 32)   128         conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_8 (UpSampling2D)  (None, 128, 128, 32) 0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 128, 128, 48) 0           up_sampling2d_8[0][0]            \n",
      "                                                                 conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 128, 128, 16) 6928        concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 128, 128, 16) 64          conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 128, 128, 1)  17          batch_normalization_29[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 245,237\n",
      "Trainable params: 244,507\n",
      "Non-trainable params: 730\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import logging, os\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import BatchNormalization, Conv2D, UpSampling2D, MaxPooling2D, Dropout\n",
    "from tensorflow.keras.optimizers import SGD, Adam, Adagrad\n",
    "import numpy as np\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def get_unet():\n",
    "    concat_axis = 3\n",
    "    inputs = layers.Input(shape = (512, 512, 1))\n",
    "\n",
    "    feats = 4#16\n",
    "    bn0 = BatchNormalization(axis=3)(inputs)\n",
    "    conv1 = layers.Conv2D(feats, (3, 3), activation='relu', padding='same', name='conv1_1')(bn0)\n",
    "    bn2 = BatchNormalization(axis=3)(conv1)\n",
    "    pool1 = layers.MaxPooling2D(pool_size=(2, 2))(bn2) #256\n",
    "\n",
    "    conv2 = layers.Conv2D(2*feats, (3, 3), activation='relu', padding='same')(pool1)\n",
    "    bn4 = BatchNormalization(axis=3)(conv2)\n",
    "    pool2 = layers.MaxPooling2D(pool_size=(2, 2))(bn4) #128\n",
    "\n",
    "    conv3 = layers.Conv2D(4*feats, (3, 3), activation='relu', padding='same')(pool2)\n",
    "    bn6 = BatchNormalization(axis=3)(conv3)\n",
    "    pool3 = layers.MaxPooling2D(pool_size=(2, 2))(bn6) #64\n",
    "\n",
    "    conv4 = layers.Conv2D(8*feats, (3, 3), activation='relu', padding='same')(pool3)\n",
    "    bn8 = BatchNormalization(axis=3)(conv4)\n",
    "    pool4 = layers.MaxPooling2D(pool_size=(2, 2))(bn8) #32\n",
    "\n",
    "    conv5 = layers.Conv2D(16*feats, (3, 3), activation='relu', padding='same')(pool4)\n",
    "    bn10 = BatchNormalization(axis=3)(conv5)\n",
    "    pool5 = layers.MaxPooling2D(pool_size=(2, 2))(bn10) #16\n",
    "\n",
    "    conv6 = layers.Conv2D(32*feats, (3, 3), activation='relu', padding='same')(pool5)\n",
    "    bn11 = BatchNormalization(axis=3)(conv6)\n",
    "\n",
    "    up_conv6 = layers.UpSampling2D(size=(2, 2))(bn11) #32\n",
    "    up7 = layers.concatenate([up_conv6, conv5], axis=concat_axis)\n",
    "\n",
    "    conv7 = layers.Conv2D(16*feats, (3, 3), activation='relu', padding='same')(up7)\n",
    "    bn13 = BatchNormalization(axis=3)(conv7)\n",
    "    \n",
    "    up_conv5 = layers.UpSampling2D(size=(2, 2))(bn13) #64\n",
    "    up6 = layers.concatenate([up_conv5, conv4], axis=concat_axis)\n",
    "\n",
    "    conv6 = layers.Conv2D(8*feats, (3, 3), activation='relu', padding='same')(up6)\n",
    "    bn15 = BatchNormalization(axis=3)(conv6)\n",
    "\n",
    "    up_conv6 = layers.UpSampling2D(size=(2, 2))(bn15) #128\n",
    "    up7 = layers.concatenate([up_conv6, conv3], axis=concat_axis)\n",
    "    \n",
    "    conv7 = layers.Conv2D(4*feats, (3, 3), activation='relu', padding='same')(up7)\n",
    "    bn13 = BatchNormalization(axis=3)(conv7)\n",
    "\n",
    "    # Rectify last convolution layer to constraint output to positive precipitation values.\n",
    "    conv8 = layers.Conv2D(1, (1, 1), activation='relu')(bn13)\n",
    "\n",
    "    model = models.Model(inputs=inputs, outputs=conv8)\n",
    "\n",
    "\n",
    "    return model\n",
    "\n",
    "model = get_unet()\n",
    "print(model.summary())\n",
    "opt = Adagrad(lr=0.0001)\n",
    "model.compile(loss='mse', optimizer=opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------8-----------\n",
      "(4229, 512, 512, 1) (4229, 128, 128, 1)\n",
      "MSE train 0.7034511\n",
      "MSE test 1.3877556\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            [(None, 512, 512, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 512, 512, 1)  4           input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_1 (Conv2D)                (None, 512, 512, 4)  40          batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 512, 512, 4)  16          conv1_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling2D) (None, 256, 256, 4)  0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 256, 256, 8)  296         max_pooling2d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 256, 256, 8)  32          conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling2D) (None, 128, 128, 8)  0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 128, 128, 16) 1168        max_pooling2d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 128, 128, 16) 64          conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling2D) (None, 64, 64, 16)   0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 64, 64, 32)   4640        max_pooling2d_17[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 64, 64, 32)   128         conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling2D) (None, 32, 32, 32)   0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 32, 32, 64)   18496       max_pooling2d_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 32, 32, 64)   256         conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling2D) (None, 16, 16, 64)   0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 16, 16, 128)  73856       max_pooling2d_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 16, 16, 128)  512         conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_9 (UpSampling2D)  (None, 32, 32, 128)  0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 32, 32, 192)  0           up_sampling2d_9[0][0]            \n",
      "                                                                 conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 32, 32, 64)   110656      concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 32, 32, 64)   256         conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_10 (UpSampling2D) (None, 64, 64, 64)   0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 64, 64, 96)   0           up_sampling2d_10[0][0]           \n",
      "                                                                 conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 64, 64, 32)   27680       concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 64, 64, 32)   128         conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_11 (UpSampling2D) (None, 128, 128, 32) 0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 128, 128, 48) 0           up_sampling2d_11[0][0]           \n",
      "                                                                 conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 128, 128, 16) 6928        concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 128, 128, 16) 64          conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 128, 128, 1)  17          batch_normalization_39[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 245,237\n",
      "Trainable params: 244,507\n",
      "Non-trainable params: 730\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Train on 4229 samples, validate on 1482 samples\n",
      "Epoch 1/200\n",
      "4229/4229 [==============================] - 16s 4ms/sample - loss: 0.9354 - val_loss: 1.3676\n",
      "Epoch 2/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.7940 - val_loss: 1.2855\n",
      "Epoch 3/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.7385 - val_loss: 1.2392\n",
      "Epoch 4/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.7061 - val_loss: 1.2225\n",
      "Epoch 5/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.6874 - val_loss: 1.2184\n",
      "Epoch 6/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.6738 - val_loss: 1.2124\n",
      "Epoch 7/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.6641 - val_loss: 1.2019\n",
      "Epoch 8/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.6566 - val_loss: 1.1939\n",
      "Epoch 9/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.6489 - val_loss: 1.1858\n",
      "Epoch 10/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.6438 - val_loss: 1.1755\n",
      "Epoch 11/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.6392 - val_loss: 1.1689\n",
      "Epoch 12/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.6342 - val_loss: 1.1638\n",
      "Epoch 13/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.6284 - val_loss: 1.1575\n",
      "Epoch 14/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.6269 - val_loss: 1.1508\n",
      "Epoch 15/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.6246 - val_loss: 1.1464\n",
      "Epoch 16/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.6208 - val_loss: 1.1444\n",
      "Epoch 17/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.6153 - val_loss: 1.1397\n",
      "Epoch 18/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.6147 - val_loss: 1.1382\n",
      "Epoch 19/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.6130 - val_loss: 1.1313\n",
      "Epoch 20/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.6106 - val_loss: 1.1290\n",
      "Epoch 21/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.6083 - val_loss: 1.1224\n",
      "Epoch 22/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.6072 - val_loss: 1.1229\n",
      "Epoch 23/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.6058 - val_loss: 1.1200\n",
      "Epoch 24/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.6019 - val_loss: 1.1165\n",
      "Epoch 25/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.6000 - val_loss: 1.1149\n",
      "Epoch 26/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.6003 - val_loss: 1.1129\n",
      "Epoch 27/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5994 - val_loss: 1.1103\n",
      "Epoch 28/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5964 - val_loss: 1.1045\n",
      "Epoch 29/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5938 - val_loss: 1.1048\n",
      "Epoch 30/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5969 - val_loss: 1.1068\n",
      "Epoch 31/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5924 - val_loss: 1.1014\n",
      "Epoch 32/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5905 - val_loss: 1.0982\n",
      "Epoch 33/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5925 - val_loss: 1.0986\n",
      "Epoch 34/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5898 - val_loss: 1.0970\n",
      "Epoch 35/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5896 - val_loss: 1.0961\n",
      "Epoch 36/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5880 - val_loss: 1.0936\n",
      "Epoch 37/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5874 - val_loss: 1.0911\n",
      "Epoch 38/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5873 - val_loss: 1.0917\n",
      "Epoch 39/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5857 - val_loss: 1.0887\n",
      "Epoch 40/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5855 - val_loss: 1.0877\n",
      "Epoch 41/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5816 - val_loss: 1.0855\n",
      "Epoch 42/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5813 - val_loss: 1.0861\n",
      "Epoch 43/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5829 - val_loss: 1.0849\n",
      "Epoch 44/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5822 - val_loss: 1.0856\n",
      "Epoch 45/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5785 - val_loss: 1.0831\n",
      "Epoch 46/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5770 - val_loss: 1.0801\n",
      "Epoch 47/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5793 - val_loss: 1.0809\n",
      "Epoch 48/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5769 - val_loss: 1.0797\n",
      "Epoch 49/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5784 - val_loss: 1.0783\n",
      "Epoch 50/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5753 - val_loss: 1.0772\n",
      "Epoch 51/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5734 - val_loss: 1.0752\n",
      "Epoch 52/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5739 - val_loss: 1.0753\n",
      "Epoch 53/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5722 - val_loss: 1.0740\n",
      "Epoch 54/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5734 - val_loss: 1.0750\n",
      "Epoch 55/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5722 - val_loss: 1.0719\n",
      "Epoch 56/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5721 - val_loss: 1.0727\n",
      "Epoch 57/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5694 - val_loss: 1.0727\n",
      "Epoch 58/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5689 - val_loss: 1.0722\n",
      "Epoch 59/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5696 - val_loss: 1.0716\n",
      "Epoch 60/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5682 - val_loss: 1.0700\n",
      "Epoch 61/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5680 - val_loss: 1.0670\n",
      "Epoch 62/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5694 - val_loss: 1.0681\n",
      "Epoch 63/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5683 - val_loss: 1.0662\n",
      "Epoch 64/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5663 - val_loss: 1.0670\n",
      "Epoch 65/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5669 - val_loss: 1.0662\n",
      "Epoch 66/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5652 - val_loss: 1.0675\n",
      "Epoch 67/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5673 - val_loss: 1.0669\n",
      "Epoch 68/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5621 - val_loss: 1.0638\n",
      "Epoch 69/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5639 - val_loss: 1.0636\n",
      "Epoch 70/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5643 - val_loss: 1.0637\n",
      "Epoch 71/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5630 - val_loss: 1.0636\n",
      "Epoch 72/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5626 - val_loss: 1.0619\n",
      "Epoch 73/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5634 - val_loss: 1.0636\n",
      "Epoch 74/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5620 - val_loss: 1.0609\n",
      "Epoch 75/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5611 - val_loss: 1.0605\n",
      "Epoch 76/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5612 - val_loss: 1.0615\n",
      "Epoch 77/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5617 - val_loss: 1.0607\n",
      "Epoch 78/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5630 - val_loss: 1.0585\n",
      "Epoch 79/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5593 - val_loss: 1.0597\n",
      "Epoch 80/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5576 - val_loss: 1.0592\n",
      "Epoch 81/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5578 - val_loss: 1.0587\n",
      "Epoch 82/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5581 - val_loss: 1.0578\n",
      "Epoch 83/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5562 - val_loss: 1.0583\n",
      "Epoch 84/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5576 - val_loss: 1.0565\n",
      "Epoch 85/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5566 - val_loss: 1.0566\n",
      "Epoch 86/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5577 - val_loss: 1.0572\n",
      "Epoch 87/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5559 - val_loss: 1.0568\n",
      "Epoch 88/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5577 - val_loss: 1.0573\n",
      "Epoch 89/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5570 - val_loss: 1.0554\n",
      "Epoch 90/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5551 - val_loss: 1.0567\n",
      "Epoch 91/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5589 - val_loss: 1.0565\n",
      "Epoch 92/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5530 - val_loss: 1.0570\n",
      "Epoch 93/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5527 - val_loss: 1.0553\n",
      "Epoch 94/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5516 - val_loss: 1.0542\n",
      "Epoch 95/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5517 - val_loss: 1.0534\n",
      "Epoch 96/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5531 - val_loss: 1.0545\n",
      "Epoch 97/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5511 - val_loss: 1.0542\n",
      "Epoch 98/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5506 - val_loss: 1.0549\n",
      "Epoch 99/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5528 - val_loss: 1.0534\n",
      "Epoch 100/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5521 - val_loss: 1.0536\n",
      "Epoch 101/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5508 - val_loss: 1.0540\n",
      "Epoch 102/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5492 - val_loss: 1.0527\n",
      "Epoch 103/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5499 - val_loss: 1.0526\n",
      "Epoch 104/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5487 - val_loss: 1.0539\n",
      "Epoch 105/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5476 - val_loss: 1.0528\n",
      "Epoch 106/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5498 - val_loss: 1.0538\n",
      "Epoch 107/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5489 - val_loss: 1.0525\n",
      "Epoch 108/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5473 - val_loss: 1.0524\n",
      "Epoch 109/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5460 - val_loss: 1.0520\n",
      "Epoch 110/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5451 - val_loss: 1.0519\n",
      "Epoch 111/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5468 - val_loss: 1.0520\n",
      "Epoch 112/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5463 - val_loss: 1.0526\n",
      "Epoch 113/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5472 - val_loss: 1.0522\n",
      "Epoch 114/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5464 - val_loss: 1.0513\n",
      "Epoch 115/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5449 - val_loss: 1.0516\n",
      "Epoch 116/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5437 - val_loss: 1.0514\n",
      "Epoch 117/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5445 - val_loss: 1.0518\n",
      "Epoch 118/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5420 - val_loss: 1.0521\n",
      "Epoch 119/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5416 - val_loss: 1.0515\n",
      "Epoch 120/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5431 - val_loss: 1.0516\n",
      "Epoch 121/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5438 - val_loss: 1.0522\n",
      "Epoch 122/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5408 - val_loss: 1.0517\n",
      "Epoch 123/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5403 - val_loss: 1.0500\n",
      "Epoch 124/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5445 - val_loss: 1.0494\n",
      "Epoch 125/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5421 - val_loss: 1.0500\n",
      "Epoch 126/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5406 - val_loss: 1.0504\n",
      "Epoch 127/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5399 - val_loss: 1.0498\n",
      "Epoch 128/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5395 - val_loss: 1.0492\n",
      "Epoch 129/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5405 - val_loss: 1.0498\n",
      "Epoch 130/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5387 - val_loss: 1.0508\n",
      "Epoch 131/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5444 - val_loss: 1.0505\n",
      "Epoch 132/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5378 - val_loss: 1.0504\n",
      "Epoch 133/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5393 - val_loss: 1.0497\n",
      "Epoch 134/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5367 - val_loss: 1.0484\n",
      "Epoch 135/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5385 - val_loss: 1.0492\n",
      "Epoch 136/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5390 - val_loss: 1.0482\n",
      "Epoch 137/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5355 - val_loss: 1.0496\n",
      "Epoch 138/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5361 - val_loss: 1.0492\n",
      "Epoch 139/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5377 - val_loss: 1.0491\n",
      "Epoch 140/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5364 - val_loss: 1.0492\n",
      "Epoch 141/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5365 - val_loss: 1.0494\n",
      "Epoch 142/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5345 - val_loss: 1.0485\n",
      "Epoch 143/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5355 - val_loss: 1.0474\n",
      "Epoch 144/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5387 - val_loss: 1.0464\n",
      "Epoch 145/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5351 - val_loss: 1.0490\n",
      "Epoch 146/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5344 - val_loss: 1.0487\n",
      "Epoch 147/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5387 - val_loss: 1.0498\n",
      "Epoch 148/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5330 - val_loss: 1.0505\n",
      "Epoch 149/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5343 - val_loss: 1.0503\n",
      "Epoch 150/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5330 - val_loss: 1.0494\n",
      "Epoch 151/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5315 - val_loss: 1.0489\n",
      "Epoch 152/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5353 - val_loss: 1.0481\n",
      "Epoch 153/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5332 - val_loss: 1.0485\n",
      "Epoch 154/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5333 - val_loss: 1.0498\n",
      "Epoch 155/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5339 - val_loss: 1.0492\n",
      "Epoch 156/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5298 - val_loss: 1.0479\n",
      "Epoch 157/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5321 - val_loss: 1.0480\n",
      "Epoch 158/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5298 - val_loss: 1.0492\n",
      "Epoch 159/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5310 - val_loss: 1.0494\n",
      "Epoch 160/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5328 - val_loss: 1.0491\n",
      "Epoch 161/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5303 - val_loss: 1.0491\n",
      "Epoch 162/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5319 - val_loss: 1.0487\n",
      "Epoch 163/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5301 - val_loss: 1.0488\n",
      "Epoch 164/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5306 - val_loss: 1.0492\n",
      "Epoch 165/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5321 - val_loss: 1.0495\n",
      "Epoch 166/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5319 - val_loss: 1.0493\n",
      "Epoch 167/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5308 - val_loss: 1.0487\n",
      "Epoch 168/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5297 - val_loss: 1.0492\n",
      "Epoch 169/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5319 - val_loss: 1.0499\n",
      "Epoch 170/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5285 - val_loss: 1.0487\n",
      "Epoch 171/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5285 - val_loss: 1.0493\n",
      "Epoch 172/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5261 - val_loss: 1.0495\n",
      "Epoch 173/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5284 - val_loss: 1.0485\n",
      "Epoch 174/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5263 - val_loss: 1.0498\n",
      "Epoch 175/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5285 - val_loss: 1.0499\n",
      "Epoch 176/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5274 - val_loss: 1.0494\n",
      "Epoch 177/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5299 - val_loss: 1.0489\n",
      "Epoch 178/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5234 - val_loss: 1.0493\n",
      "Epoch 179/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5266 - val_loss: 1.0508\n",
      "Epoch 180/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5250 - val_loss: 1.0498\n",
      "Epoch 181/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5273 - val_loss: 1.0497\n",
      "Epoch 182/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5271 - val_loss: 1.0495\n",
      "Epoch 183/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5253 - val_loss: 1.0491\n",
      "Epoch 184/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5259 - val_loss: 1.0495\n",
      "Epoch 185/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5243 - val_loss: 1.0490\n",
      "Epoch 186/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5264 - val_loss: 1.0487\n",
      "Epoch 187/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5260 - val_loss: 1.0478\n",
      "Epoch 188/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5242 - val_loss: 1.0479\n",
      "Epoch 189/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5213 - val_loss: 1.0474\n",
      "Epoch 190/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5213 - val_loss: 1.0480\n",
      "Epoch 191/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5237 - val_loss: 1.0485\n",
      "Epoch 192/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5233 - val_loss: 1.0481\n",
      "Epoch 193/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5240 - val_loss: 1.0493\n",
      "Epoch 194/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5278 - val_loss: 1.0482\n",
      "Epoch 195/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5241 - val_loss: 1.0481\n",
      "Epoch 196/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5245 - val_loss: 1.0480\n",
      "Epoch 197/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5226 - val_loss: 1.0495\n",
      "Epoch 198/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5241 - val_loss: 1.0489\n",
      "Epoch 199/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5236 - val_loss: 1.0492\n",
      "Epoch 200/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5235 - val_loss: 1.0488\n",
      "-----------9-----------\n",
      "(4229, 512, 512, 1) (4229, 128, 128, 1)\n",
      "MSE train 0.70345676\n",
      "MSE test 1.386821\n",
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            [(None, 512, 512, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 512, 512, 1)  4           input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_1 (Conv2D)                (None, 512, 512, 4)  40          batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 512, 512, 4)  16          conv1_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling2D) (None, 256, 256, 4)  0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 256, 256, 8)  296         max_pooling2d_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 256, 256, 8)  32          conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_21 (MaxPooling2D) (None, 128, 128, 8)  0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 128, 128, 16) 1168        max_pooling2d_21[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 128, 128, 16) 64          conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_22 (MaxPooling2D) (None, 64, 64, 16)   0           batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 64, 64, 32)   4640        max_pooling2d_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 64, 64, 32)   128         conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_23 (MaxPooling2D) (None, 32, 32, 32)   0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 32, 32, 64)   18496       max_pooling2d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 32, 32, 64)   256         conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_24 (MaxPooling2D) (None, 16, 16, 64)   0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 16, 16, 128)  73856       max_pooling2d_24[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 16, 16, 128)  512         conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_12 (UpSampling2D) (None, 32, 32, 128)  0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 32, 32, 192)  0           up_sampling2d_12[0][0]           \n",
      "                                                                 conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 32, 32, 64)   110656      concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 32, 32, 64)   256         conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_13 (UpSampling2D) (None, 64, 64, 64)   0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)    (None, 64, 64, 96)   0           up_sampling2d_13[0][0]           \n",
      "                                                                 conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 64, 64, 32)   27680       concatenate_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 64, 64, 32)   128         conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_14 (UpSampling2D) (None, 128, 128, 32) 0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_14 (Concatenate)    (None, 128, 128, 48) 0           up_sampling2d_14[0][0]           \n",
      "                                                                 conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 128, 128, 16) 6928        concatenate_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 128, 128, 16) 64          conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 128, 128, 1)  17          batch_normalization_49[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 245,237\n",
      "Trainable params: 244,507\n",
      "Non-trainable params: 730\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4229 samples, validate on 1483 samples\n",
      "Epoch 1/200\n",
      "4229/4229 [==============================] - 15s 3ms/sample - loss: 0.9264 - val_loss: 1.3575\n",
      "Epoch 2/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.7843 - val_loss: 1.2527\n",
      "Epoch 3/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.7348 - val_loss: 1.2117\n",
      "Epoch 4/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.7040 - val_loss: 1.2062\n",
      "Epoch 5/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.6843 - val_loss: 1.2065\n",
      "Epoch 6/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.6687 - val_loss: 1.2014\n",
      "Epoch 7/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.6571 - val_loss: 1.1894\n",
      "Epoch 8/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.6476 - val_loss: 1.1798\n",
      "Epoch 9/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.6388 - val_loss: 1.1700\n",
      "Epoch 10/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.6334 - val_loss: 1.1586\n",
      "Epoch 11/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.6288 - val_loss: 1.1515\n",
      "Epoch 12/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.6230 - val_loss: 1.1457\n",
      "Epoch 13/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.6165 - val_loss: 1.1385\n",
      "Epoch 14/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.6148 - val_loss: 1.1308\n",
      "Epoch 15/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.6119 - val_loss: 1.1259\n",
      "Epoch 16/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.6085 - val_loss: 1.1237\n",
      "Epoch 17/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.6016 - val_loss: 1.1194\n",
      "Epoch 18/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.6024 - val_loss: 1.1170\n",
      "Epoch 19/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.6000 - val_loss: 1.1102\n",
      "Epoch 20/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5971 - val_loss: 1.1086\n",
      "Epoch 21/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5950 - val_loss: 1.1026\n",
      "Epoch 22/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5935 - val_loss: 1.1019\n",
      "Epoch 23/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5920 - val_loss: 1.0986\n",
      "Epoch 24/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5879 - val_loss: 1.0956\n",
      "Epoch 25/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5858 - val_loss: 1.0931\n",
      "Epoch 26/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5861 - val_loss: 1.0920\n",
      "Epoch 27/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5858 - val_loss: 1.0892\n",
      "Epoch 28/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5822 - val_loss: 1.0840\n",
      "Epoch 29/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5799 - val_loss: 1.0851\n",
      "Epoch 30/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5828 - val_loss: 1.0863\n",
      "Epoch 31/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5784 - val_loss: 1.0803\n",
      "Epoch 32/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5770 - val_loss: 1.0774\n",
      "Epoch 33/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5779 - val_loss: 1.0785\n",
      "Epoch 34/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5761 - val_loss: 1.0775\n",
      "Epoch 35/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5754 - val_loss: 1.0764\n",
      "Epoch 36/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5743 - val_loss: 1.0743\n",
      "Epoch 37/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5730 - val_loss: 1.0725\n",
      "Epoch 38/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5729 - val_loss: 1.0726\n",
      "Epoch 39/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5711 - val_loss: 1.0699\n",
      "Epoch 40/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5713 - val_loss: 1.0688\n",
      "Epoch 41/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5677 - val_loss: 1.0664\n",
      "Epoch 42/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5681 - val_loss: 1.0666\n",
      "Epoch 43/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5685 - val_loss: 1.0659\n",
      "Epoch 44/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5680 - val_loss: 1.0678\n",
      "Epoch 45/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5651 - val_loss: 1.0646\n",
      "Epoch 46/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5630 - val_loss: 1.0615\n",
      "Epoch 47/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5653 - val_loss: 1.0625\n",
      "Epoch 48/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5630 - val_loss: 1.0619\n",
      "Epoch 49/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5642 - val_loss: 1.0608\n",
      "Epoch 50/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5611 - val_loss: 1.0602\n",
      "Epoch 51/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5594 - val_loss: 1.0579\n",
      "Epoch 52/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5599 - val_loss: 1.0578\n",
      "Epoch 53/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5587 - val_loss: 1.0564\n",
      "Epoch 54/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5586 - val_loss: 1.0571\n",
      "Epoch 55/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5578 - val_loss: 1.0545\n",
      "Epoch 56/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5573 - val_loss: 1.0554\n",
      "Epoch 57/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5557 - val_loss: 1.0552\n",
      "Epoch 58/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5551 - val_loss: 1.0552\n",
      "Epoch 59/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5552 - val_loss: 1.0556\n",
      "Epoch 60/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5548 - val_loss: 1.0540\n",
      "Epoch 61/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5537 - val_loss: 1.0515\n",
      "Epoch 62/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5550 - val_loss: 1.0520\n",
      "Epoch 63/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5545 - val_loss: 1.0506\n",
      "Epoch 64/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5520 - val_loss: 1.0509\n",
      "Epoch 65/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5526 - val_loss: 1.0505\n",
      "Epoch 66/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5511 - val_loss: 1.0513\n",
      "Epoch 67/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5528 - val_loss: 1.0510\n",
      "Epoch 68/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5484 - val_loss: 1.0484\n",
      "Epoch 69/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5491 - val_loss: 1.0480\n",
      "Epoch 70/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5504 - val_loss: 1.0479\n",
      "Epoch 71/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5493 - val_loss: 1.0484\n",
      "Epoch 72/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5480 - val_loss: 1.0465\n",
      "Epoch 73/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5495 - val_loss: 1.0480\n",
      "Epoch 74/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5485 - val_loss: 1.0465\n",
      "Epoch 75/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5468 - val_loss: 1.0457\n",
      "Epoch 76/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5472 - val_loss: 1.0464\n",
      "Epoch 77/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5480 - val_loss: 1.0462\n",
      "Epoch 78/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5489 - val_loss: 1.0444\n",
      "Epoch 79/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5453 - val_loss: 1.0451\n",
      "Epoch 80/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5435 - val_loss: 1.0444\n",
      "Epoch 81/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5442 - val_loss: 1.0444\n",
      "Epoch 82/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5441 - val_loss: 1.0436\n",
      "Epoch 83/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5420 - val_loss: 1.0447\n",
      "Epoch 84/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5437 - val_loss: 1.0426\n",
      "Epoch 85/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5426 - val_loss: 1.0430\n",
      "Epoch 86/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5434 - val_loss: 1.0431\n",
      "Epoch 87/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5428 - val_loss: 1.0428\n",
      "Epoch 88/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5446 - val_loss: 1.0430\n",
      "Epoch 89/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5436 - val_loss: 1.0415\n",
      "Epoch 90/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5414 - val_loss: 1.0419\n",
      "Epoch 91/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5448 - val_loss: 1.0423\n",
      "Epoch 92/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5395 - val_loss: 1.0432\n",
      "Epoch 93/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5387 - val_loss: 1.0413\n",
      "Epoch 94/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5379 - val_loss: 1.0408\n",
      "Epoch 95/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5384 - val_loss: 1.0398\n",
      "Epoch 96/200\n",
      "4229/4229 [==============================] - 13s 3ms/sample - loss: 0.5396 - val_loss: 1.0402\n",
      "Epoch 97/200\n",
      "1216/4229 [=======>......................] - ETA: 8s - loss: 0.4851"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from numpy.random import seed\n",
    "\n",
    "for b in range(8,17):\n",
    "    seed(1)\n",
    "    tf.random.set_seed(2)\n",
    "    print(f\"-----------{b}-----------\")\n",
    "    x_train,y_train,x_test,y_test = get_band_ds(b)\n",
    "    print(x_train.shape, y_train.shape)\n",
    "\n",
    "    print(\"MSE train\", np.mean(np.square(y_train)))\n",
    "    print(\"MSE test\", np.mean(np.square(y_test)))\n",
    "    \n",
    "    model = get_unet()\n",
    "    print(model.summary())\n",
    "    model.compile(loss='mse', optimizer=Adagrad(lr=0.0001))\n",
    "    \n",
    "    history = model.fit(x_train, y_train, validation_data=(x_test, y_test), shuffle=True, epochs=200)\n",
    "    \n",
    "    with open(f'history_3months_100epochs_4chan_b{b}.pkl', 'wb') as f:\n",
    "        pickle.dump(history.history, f)\n",
    "\n",
    "    model.save(f'model_3months_100epochs_4chan_b{b}.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "#200 1\n",
    "res = np.array([1.0488,1.0349,1.0106,0.9807,0.9958,0.9834,0.9940,0.9971,1.0128])\n",
    "#100 1\n",
    "#res = np.array([1.0371,1.0770,1.0187,0.9976,1.0232,1.0022,1.0058,1.0338,1.0162])\n",
    "\n",
    "res -= res.min()\n",
    "res /= res.max()\n",
    "\n",
    "#plt.figure(figsize=(20,10))\n",
    "plt.bar(list(range(8,17)),res,tick_label=[\"B8\",\"B9\",\"B10\",\"B11\",\"B12\",\"B13\",\"B14\",\"B15\",\"B16\"])\n",
    "plt.title(\"Relative error: Himawari8 Band -> GPM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
